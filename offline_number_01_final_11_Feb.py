# -*- coding: utf-8 -*-
"""OFFLINE_NUMBER_01_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rw2Oq9lFjEPzGcc1XC7uIBingTc2KjhE
"""

# KNN algo, Sklearn, Dataset extraction with only python
#The iris dataset has been provided in the drive

!ls

#how to upload file from your computer in google colab
from google.colab import files
uploaded = files.upload()

!ls

#reading the file lines
lines=[]
#change name
with open('iris_dataset.txt') as f:
    lines = f.read().splitlines()
    
print(lines[0:150])

# splitting each sample into attributes
wanted_lines = []
for i in range(0,len(lines)):
  line = lines[i].split(",")
  wanted_lines.append(line)
  
print(wanted_lines[0:150])

# getting a view at the unique outputs
outputs = []
for i in range(0,len(wanted_lines)):
  outputs.append(wanted_lines[i][-1])
output_set = set(outputs)
print(output_set)

# converting string outputs to values and keeping in dictionary
output_dict={}
i=0
for element in output_set:
  print(element)
  output_dict[element] = i
  i=i+1
print(output_dict)

# replacing string outputs with dictionary ints
for i in range(0,len(wanted_lines)):
  wanted_lines[i][-1] = output_dict[wanted_lines[i][-1]]
print(wanted_lines)

# Converting all string attributes into floats
import numpy as np

for i in range(0,len(wanted_lines)):
  for j in range(0,len(wanted_lines[0])):
    wanted_lines[i][j] = float(wanted_lines[i][j])

print(np.array(wanted_lines))

#Train, test, validation split with probability

import random

Train = []
Test = []
Validation = []

for i in range(0, len(wanted_lines)):
  num = random.uniform(0, 1)
  if(num>0.7 and num <= 0.85):
    Validation.append(wanted_lines[i])
  elif(num>0.85 and num<=1.0):
    Test.append(wanted_lines[i])
  else:
    Train.append(wanted_lines[i])

print(len(Train))
print(len(Test))
print(len(Validation))

#numpy array conversion

train = np.array(Train)
validation = np.array(Validation)
test = np.array(Test)

print("Train Set\n",train,"\n")
print("Validation Set\n",validation,"\n")
print("Test Set\n",test,"\n")
#These will be our train, test and validation set

# Run the above cells only once for getting these 3 sets. Then write the KNN algo

import math
from collections import Counter
#Once initialized, counters are accessed just like dictionaries. Also, it does not raise the KeyValue error (if key is not present) instead the valueâ€™s count is shown as 0.


actual=0
sum=0.0
 
K=30

for i in validation:
  distance_class=[]
  for j in train:
    distance=math.sqrt(pow(i[0]-j[0],2)+pow(i[1]-j[1],2)+pow(i[2]-j[2],2)+pow(i[3]-j[3],2))
    distance_class.append([distance,j[4]])
  distance_class=sorted(distance_class)
 # print("The Distance Class->>>>\n",distance_class,"\n")
  nearest_k_neighbour=[]
  for x in range(K):
    nearest_k_neighbour.append(distance_class[x])
  nearest_k_neighbour=np.array(nearest_k_neighbour)
  majority=Counter(nearest_k_neighbour.flat).most_common(1)
  #Return a list of the n most common elements and their counts from the most common to the least
  #1_D iterator over N-dimensional arrays (.flat).
  #print(majority[0][0])
  
  
  
  
  #for a in range(K):
    #sum=sum+distance_class[a].[0]
  
  
  
  if majority[0][0] == i[4] :
    actual=actual+1
    
    print("Actual Value",actual)
accuracy=(actual/len(validation))*100
print("Accuracy For Classification: ",accuracy)
print("Accuracy For Regression: ",sum/K)

correct=0
K=30
for i in test:
  distance_class=[]
  for j in train:
    distance=math.sqrt(pow(i[0]-j[0],2)+pow(i[1]-j[1],2)+pow(i[2]-j[2],2)+pow(i[3]-j[3],2))
    distance_class.append([distance,j[4]])
  distance_class=sorted(distance_class)
  #print(distance_class)
  nearest_k_neighbour=[]
  for x in range(K):
    nearest_k_neighbour.append(distance_class[x])
  nearest_k_neighbour=np.array(nearest_k_neighbour)
  majority=Counter(nearest_k_neighbour.flat).most_common(1)
  
  if majority[0][0] == i[4] :
    correct=correct+1
accuracy=(correct/len(test))*100
print("Accuracy For Classification : ",accuracy)