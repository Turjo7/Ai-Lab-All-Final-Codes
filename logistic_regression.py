# -*- coding: utf-8 -*-
"""Copy of Logistic_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSjpt7pgBLME35v6cdIaW1eiIFGcftBN
"""

#Data preprocessing starts here
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

iris = datasets.load_iris()

x = iris.data[:, :2]
y = (iris.target != 0) * 1
#print(X)
#x1=X[ : ,-0]
#x1=np.array(x1)
#print(x1)
#x2=X[ : ,-1]
#x2=np.array(x2)
#print(x2)

#x3=[1.0,1.0,1.0,1.0]
#print(x3)  

#x3=np.array(x3)
#X=[x3,x1,x2]
#X.insert(0,1)
#X=np.array(X)

#print(X)
print(y)

import random
x= iris.data[:, :2]
y = (iris.target != 0) * 1
Train = []
Test = []
Validation = []

for i in range(0, len(x)):
  num = random.uniform(0, 1)
  if(num>0.7 and num <= 0.85):
    Validation.append(x[i])
  elif(num>0.85 and num<=1.0):
    Test.append(x[i])
  else:
    Train.append(x[i])

print(len(Train))
print(len(Test))
print(len(Validation))

train = np.array(Train)
validation = np.array(Validation)
test = np.array(Test)

print(train)
print(validation)
print(test)

import random
x= iris.data[:, :2]
y = (iris.target != 0) * 1
Train_y = []
Test_y = []
Validation_y = []

for i in range(0, len(y)):
  num = random.uniform(0, 1)
  if(num>0.7 and num <= 0.85):
    Validation_y.append(y[i])
  elif(num>0.85 and num<=1.0):
    Test_y.append(y[i])
  else:
    Train_y.append(y[i])

print(len(Train_y))
print(len(Test_y))
print(len(Validation_y))

train_y = np.array(Train_y)
validation_y = np.array(Validation_y)
test_y = np.array(Test_y)

print(train_y)
print(validation_y)
print(test_y)

#x1=train[ : ,-0]
#x1=np.array(x1)
#print(x1)

#x2=train[ : ,-1]
#x2=np.array(x2)
#print(x2)
#print(len(x2))

#X=[x3, x1, x2]
#print(X)
#X=np.array(X)
#print(X)

X=[1,0.6,0.35 ]
X=np.array(X)
print(X)

#THETA= np.random.random((1,3))
#print(THETA)
#theta_3=THETA[:,-3]
#print(theta_3)

#theta_1=THETA[:,-2]
#print(theta_1)

#theta_2=THETA[:,-1]
#print(theta_2)

#theta=[theta_3,theta_1,theta_2]
#theta=np.array(theta)
#print(theta)

theta3=0.03
theta1=0.003
theta2=0.02

theta=[theta3,theta1,theta2]
print(theta)

import math

X_theta=X.dot(theta)
print(X_theta)

import math

h=1.0/(1+np.exp(-X_theta))
h=np.array(h)
print(h)
h1=math.log(h,10)
print(h1)
h2=math.log(1-h,10)
print(h2)

if h >= 0.5: 
  print ("h is in class 1")
else:
    print("h is in class 0")

def_loss=(r'$\delta_j / \delta_theta"$')
print(def_loss)

import sympy as sp
loss=sp.Symbol('loss')
print(sp.diff())

from scipy.misc import derivative
def f(loss):
  return loss

for b in range(len(train_y)):
      loss=(-train_y[b]*h1-(1-train_y[b])*h2)
      print("loss is",loss)

for i in range(len(train_y)):
  a=train_y[i]
  #print(a)
  def_loss=X*(h-a)
  
  
print(def_loss)

m=1/(len(train))
print(m)

l_r=0.1
for i in range(150):
  total_loss=0
  total_def_loss=0
  
  for j in range(len(train)):
    for b in range(len(train_y)):
      loss=(-train_y[b]*h1-(1-train_y[b])*h2)
      print("loss is",loss)
      def_loss=X*(h-train_y[b])
      total_loss=total_loss+loss
      total_def_loss=total_def_loss + def_loss
      avg_loss = (1/m)*total_loss
      avg_def_loss=(1/m)*total_def_loss
      theta = theta - l_r * def_loss
      print(theta)
      print(avg_loss)

#for validation
  
  for i in range(len(validation)):
    h=1.0/(1+np.exp(-X_theta))
    h=np.array(h)
    print(h)
    
    if h >= 0.5:
      print(h,"is in class 1")
    else:
      print(h,"is in class 0")

# for valid accuacry

correct=0.0

for i in validation_y:
  
  if h == validation_y[i] :
    correct=correct+1.0
    
    
    accuracy=(correct/len(validation_y))*100
    print(accuracy)

#for test 


  for i in range(len(test)):
    h=1.0/(1+np.exp(-X_theta))
    h=np.array(h)
    print(h)
    
    if h >= 0.5:
      print(h,"is in class 1")
    else:
      print(h,"is in class 0")

# for test accuacry

correct=0.0

for i in test_y:
  
  if h == test_y[i] :
    correct=correct+1.0
    
    
    accuracy=(correct/len(test_y))*100
    print(accuracy)

#graph

plt.xlabel('iteration')
plt.ylabel('avg loss')
plt.title('logistic regression')
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')
plt.legend();
#Data preprocessing ends here

#Do not even think about copying from here
class LogisticRegression:
    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=True):
        self.lr = lr
        self.num_iter = num_iter
        self.fit_intercept = fit_intercept
        self.verbose = verbose
    
    def __add_intercept(self, X):
        intercept = np.ones((X.shape[0], 1))
        return np.concatenate((intercept, X), axis=1)
    
    def __sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    def __loss(self, h, y):
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    
    def fit(self, X, y):
        if self.fit_intercept:
            X = self.__add_intercept(X)
        # weights initialization
        self.theta = np.zeros(X.shape[1])
        
        for i in range(self.num_iter):
            z = np.dot(X, self.theta)
            h = self.__sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / y.size
            self.theta -= self.lr * gradient
            
            z = np.dot(X, self.theta)
            h = self.__sigmoid(z)
            loss = self.__loss(h, y)
                
            if(self.verbose ==True and i % 10000 == 0):
                print(f'loss: {loss} \t')
    
    def predict_prob(self, X):
        if self.fit_intercept:
            X = self.__add_intercept(X)
    
        return self.__sigmoid(np.dot(X, self.theta))
    
    def predict(self, X):
        return self.predict_prob(X).round()

model = LogisticRegression(lr=0.1, num_iter=300000)

# %time model.fit(X, y)

preds = model.predict(X)
#(preds == y).mean()
preds == y

model.theta